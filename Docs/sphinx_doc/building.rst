.. _Building:

Building
--------

ERF can be built using either GNU Make or CMake.

GNU Make
~~~~~~~~

The GNU Make system is best for use on large computing facility machines and production runs. With the GNU Make implementation, the build system will inspect the machine and use known compiler optimizations explicit to that machine if possible. These explicit settings are kept up-to-date by the AMReX project.

Using the GNU Make build system involves first setting environment variables for the directories of the dependencies of ERF which is the repository of AMReX. AMReX is provided as a git submodule in ERF and can be populated by using ``git submodule init; git submodule update`` in the ERF repo, or before cloning by using ``git clone --recursive <erf_repo>``. Although submodules of these projects are provided, they can be placed externally as long as the ``<REPO_HOME>`` environment variables for each dependency is set correctly. An example of setting the ``<REPO_HOME>`` environment variables in the user's ``.bashrc`` is shown below:

::

   export ERF_HOME=${HOME}/ERF
   export AMREX_HOME=${ERF_HOME}/Submodules/AMReX

Note that one could also use an external version of AMReX, downloaded by typing

   .. code:: shell

             git clone https://github.com/amrex-codes/amrex.git

and then, if using bash shell,

::

   export AMREX_HOME=/path/to/external/amrex

or if using tcsh,

::

   setenv AMREX_HOME /path/to/external/amrex

#. ``cd`` to the desired build directory, e.g.  ``ERF/Exec/IsentropicVortex/``

#. Edit the ``GNUmakefile``; options include

   +-----------------+------------------------------+------------------+-------------+
   | Option name     | Description                  | Possible values  | Default     |
   |                 |                              |                  | value       |
   +=================+==============================+==================+=============+
   | COMP            | Compiler (gnu or intel)      | gnu / intel      | None        |
   +-----------------+------------------------------+------------------+-------------+
   | USE_MPI         | Whether to enable MPI        | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | USE_OMP         | Whether to enable OpenMP     | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | USE_CUDA        | Whether to enable CUDA       | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | DEBUG           | Whether to use DEBUG mode    | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | PROFILE         | Include profiling info       | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | TINY_PROFILE    | Include tiny profiling info  | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | COMM_PROFILE    | Include comm profiling info  | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+
   | TRACE_PROFILE   | Include trace profiling info | TRUE / FALSE     | FALSE       |
   +-----------------+------------------------------+------------------+-------------+

   .. note::
      **Do not set both USE_OMP and USE_CUDA to true.**

   Information on using other compilers can be found in the AMReX documentation at
   https://amrex-codes.github.io/amrex/docs_html/BuildingAMReX.html .

#. Make the executable by typing

   .. code:: shell

      make

   The name of the resulting executable (generated by the GNUmake system) encodes several of the build characteristics, including dimensionality of the problem, compiler name, and whether MPI and/or OpenMP were linked with the executable.
   Thus, several different build configurations may coexist simultaneously in a problem folder.
   For example, the default build in ``ERF/Exec/Isntropic`` will look
   like ``ERF3d.gnu.MPI.ex``, indicating that this is a 3-d version of the code, made with
   ``COMP=gnu``, and ``USE_MPI=TRUE``.

Job info
~~~~~~~~

The build information can be accessed by typing

   .. code:: shell

      ./ERF*ex --describe

in the directory where the executable has been built.


CMake
~~~~~

CMake is often preferred by developers of ERF; CMake allows for building as well as easy testing and verification of ERF through the use of CTest which is included in CMake.

Using CMake involves an additional configure step before using the ``make`` command. It is also expected that the user has cloned the ERF repo with the ``--recursive`` option or performed ``git submodule init; git submodule update`` in the ERF repo to populate its submodules.

To build with CMake, a user typically creates a ``build`` directory in the project directory and in that directory the ``cmake <options> ..`` command is used to configure the project before building it. ERF provides an example build directory called ``Build`` with example scripts for performing the CMake configure. Once the CMake configure step is done, then the ``make`` command will build the executable.

An example CMake configure command to build ERF with MPI is listed below:

::

    cmake -DCMAKE_BUILD_TYPE:STRING=Release \
          -DERF_ENABLE_MPI:BOOL=ON \
          -DCMAKE_CXX_COMPILER:STRING=mpicxx \
          -DCMAKE_C_COMPILER:STRING=mpicc \
          -DCMAKE_Fortran_COMPILER:STRING=mpifort \
          .. && make

Note that CMake is able to generate makefiles for the Ninja build system as well which will allow for faster building of the executable(s).


Perlmutter (NERSC)
~~~~~~~~~~~~~~~~~~

Recall the GNU Make system is best for use on large computing facility machines and production runs. With the GNU Make implementation, the build system will inspect the machine and use known compiler optimizations explicit to that machine if possible. These explicit settings are kept up-to-date by the AMReX project.

For Perlmutter at NERSC, look at the general instructions for building ERF using GNU Make, and then you can initialize your environment by loading these modules:

::

   module load PrgEnv-gnu
   module load cudatoolkit

Then build ERF as, for example (specify your own path to the AMReX submodule in `ERF/Submodules/AMReX`):

::

   make -j 4 COMP=gnu USE_MPI=TRUE USE_OMP=FALSE USE_CUDA=TRUE AMREX_HOME=/global/u2/d/dwillcox/dev-erf/ERF/Submodules/AMReX USE_SUNDIALS=FALSE

Finally, you can prepare your SLURM job script, using the following as a guide:

   .. code:: shell

             #!/bin/bash

             ## specify your allocation (with the _g) and that you want GPU nodes
             #SBATCH -A m4106_g
             #SBATCH -C gpu

             ## the job will be named "ERF" in the queue and will save stdout to erf_[job ID].out
             #SBATCH -J ERF
             #SBATCH -o erf_%j.out

             ## set the max walltime
             #SBATCH -t 10

             ## specify the number of nodes you want
             #SBATCH -N 2

             ## we use the same number of MPI ranks per node as GPUs per node
             #SBATCH --ntasks-per-node=4

             ## assign 1 MPI rank per GPU on each node
             #SBATCH --gpus-per-task=1
             #SBATCH --gpu-bind=map_gpu:0,1,2,3

             # the -n argument is (--ntasks-per-node) * (-N) = (number of MPI ranks per node) * (number of nodes)
             srun -n 8 ./ERF3d.gnu.MPI.CUDA.ex inputs_wrf_baseline max_step=100

To submit your job script, do `sbatch [your job script]` and you can check its status by doing `squeue -u [your username]`.

